---
title: "Milestone Report"
output:
  html_document:
    df_print: paged
---

# Exploratory Analysis for text

```{r setup, message=FALSE}
library(tm)
library(tidyverse)
library(stringr)
library(ggplot2)
library(tidytext)
```

## Demonstrate that you've downloaded the data and have successfully loaded it in

Files present in dataset:

```{r load_data}
src <- file.path("data", "en_US")
en_us <- VCorpus(DirSource(src, encoding = "UTF-8"),
                readerControl = list(language = "en"))
inspect(en_us)
```

```{r read_partial}
tokenize_file <- function(f) {
  con <- file(f)
  all <- readLines(con, -1)
  close(con)
  set.seed(1)
  sample_keep <- rbinom(length(all), 1, 0.01) == 1
  sample <- all[sample_keep]
  str_extract_all(sample, "[a-zA-Z]+")
}

tokens <- tokenize_file(file.path("data", "en_us", "en_US.blogs.txt"))
```

```{r}
get_content <- function(f, n_samples = NULL) {
  con <- file(f)
  all <- readLines(con, -1, encoding="UTF-8")
  close(con)
  if (is.null(n_samples)) {
    output <- all
  } else{
    set.seed(1)
    output <- sample(all, n_samples)
  }
  tibble(text = output, book = f)
}

results <- get_content(file.path("data", "en_us", "en_US.blogs.txt"), 100000) 

clean_blogtext <- function(text) {
  text %>% 
    tolower %>%
    str_replace_all(",", " ") %>% 
    str_replace_all("(?<=\\w)[’']d(?!\\w)", " would") %>%
    str_replace_all("(?<!\\w)[Tt]here[’']s(?!\\w)", "there is") %>%
    str_replace_all("(?<!\\w)[hH]ere[’']s(?!\\w)", "here is") %>%
    str_replace_all("(?<!\\w)[tT]hat[’']s(?!\\w)", "that is") %>%
    str_replace_all("(?<!\\w)[wW]hat[’']s(?!\\w)", "what is") %>%
    str_replace_all("(?<!\\w)[wW]ho[’']s(?!\\w)", "who is") %>%
    str_replace_all("(?<!\\w)[hH]e[’']s(?!\\w)", "he is") %>%
    str_replace_all("(?<!\\w)[Ss]he[’']s(?!\\w)", "she is") %>%
    str_replace_all("(?<!\\w)[iI]t[’']s(?!\\w)", "it is") %>%
        str_replace_all("(?<!\\w)[lL]et[’']s(?!\\w)", "let us") %>%
    
    str_replace_all("(?<=\\w)n[’']t(?!\\w)", " not") %>%
    str_replace_all("(?<=\\w)[’']ve(?!\\w)", " have") %>%
    str_replace_all("(?<=\\w)[’']l{1,2}(?!\\w)", " will") %>%
    str_replace_all("(?<=\\w)[’']re{0,1}(?!\\w)", " are") %>%
    str_replace_all("(?<=\\w)[’']m(?!\\w)", " am") %>%
    
    str_replace_all("(?<=\\w)[’']s(?!\\w)", "") %>%

    str_replace_all("[^a-zA-Z ]+", ".")
}

clean <- results %>% 
  mutate(clean_text = clean_blogtext(text))

split_sentences <- str_split(clean$clean_text, "\\.") %>% 
  lapply(function(x) {data.frame(text = unlist(x), stringsAsFactors = FALSE)}) %>% 
  bind_rows %>%
  as.tibble

bigrams <- split_sentences %>% 
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  count(bigram, sort = TRUE)

trigrams <- split_sentences %>% 
  unnest_tokens(trigram, text, token = "ngrams", n = 3) %>%
  count(trigram, sort = TRUE)

fourgrams <- split_sentences %>% 
  unnest_tokens(fourgram, text, token = "ngrams", n = 4) %>%
  count(fourgram, sort = TRUE)

cleaned_sample <- sample %>% 
  str_replace_all("[’']s", " ") %>%
  str_replace_all("[’']m", " am") %>%
  str_replace_all("[^a-zA-Z\\.;!:\\(\\)\\[\\]]", " ") %>% # Sentence breaks, does not include commas
  str_replace_all(" +", " ") 

split_sample <- cleaned_sample %>% 
  str_split("[\\.;!:\\(\\)\\[\\]]") %>%
  unlist 

df <- data.frame(text = split_sample, stringsAsFactors = FALSE) %>%
  mutate(text = str_replace(text, "^ +", ""),
         text = str_replace(text, " +$", ""),
         text = str_replace(text, " +", " "),
         words = str_count(text, "\\w+")) %>%
  filter(words > 2) %>%
  as.tibble %>%
  
```


```{r remove_profanities}
profanities <- read.table(file.path("data", "swearWords.csv"), header = FALSE, stringsAsFactors = FALSE) %>%
  unlist

corp <- VCorpus(VectorSource(tokens))
clean_corp <- corp %>% tm_map(removeWords, profanities)

tdm <- TermDocumentMatrix(clean_corp)
```
## Create a basic report of summary statistics about the data sets.

### Stats

```{r line_counts}
suppressWarnings(
  word_count <- lapply(en_us, function(x) 
    sum(unlist(map(x, 
                   ~str_count(., '\\w+')))))
)

line_count <- lapply(en_us, function(x) length(x$content))

df <- data.frame(file = names(word_count), word_count = unlist(word_count), line_count = unlist(line_count))

df %>% ggplot(aes(y = word_count, x = file, fill = file)) + 
  geom_col() + 
  labs(title = "Word count in files")

df %>% ggplot(aes(y = line_count, x = file, fill = file)) + 
  geom_col() + 
  labs(title = "Line count in files")
```

### Samples from each source

#### Blogs

```{r}
head(en_us[[1]]$content, 5)
```

#### News

```{r}
head(en_us[[2]]$content, 5)
```

#### Twitter

```{r}
head(en_us[[3]]$content, 5)
```

### Sample terms in the documents

```{r}
inspect(tdm)
```

## Goal for app

I plan to make an interactive Shiny app to predict words. The user can key in a sentence and it will predict the next word in the sentence. I plan to use ngrams for the predictive algorithm, with longer ngrams having higher weight